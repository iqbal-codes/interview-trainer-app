# API Endpoint Specifications (Phase 5 Update)

This document updates and details specifications for API routes critical in Phase 5, focusing on full LLM integration for question generation and the Vapi assistant webhook. Refer to previous API specs for other endpoints.

## 1. `/api/interviews/generate` (Updated for Full LLM Integration)

- **Purpose:** Generates interview questions using a real LLM (via Vercel AI SDK with Google Generative AI) based on user inputs and initiates an interview session.
- **HTTP Method:** `POST`
- **Route Path:** `/api/interviews/generate`
- **Authentication:** Required.
- **Request Body:** (Same as before)
  ```json
  {
    "target_role": "Software Engineer",
    "key_skills_focused": ["React", "Node.js"], // Expecting array of strings now
    "interview_type": "Behavioral",
    "job_description_context": "Optional job description text...",
    "requested_num_questions": 5
  }
  ```
- **Core Logic (Updated):**

  1.  Verify user authentication. Get `user_id`.
  2.  Validate input parameters. Ensure `key_skills_focused` is an array.
  3.  Create a new entry in the `interview_sessions` table with status 'pending' and the provided details. Get the `session_id`.
  4.  **LLM Interaction (Full Implementation using Vercel AI SDK & Google Generative AI):**

      - **Setup:** Ensure `@ai-sdk/google` and `ai` packages are installed. Initialize the Google Generative AI provider (e.g., `createGoogleGenerativeAI()`).
      - Retrieve user's profile context (e.g., `profiles.cv_text_content` if available).
      - Construct a detailed prompt string based on the request body (target role, skills, type, job description, number of questions) and the user's profile/CV context. The prompt should instruct the LLM to return a list of questions, perhaps as a JSON formatted string or a numbered list.
      - **Make an API call using the Vercel AI SDK:** Use a function like `generateText` (from the `ai` package) with your initialized Google provider and the constructed prompt.

        ```typescript
        // Conceptual example within the API route
        // import { createGoogleGenerativeAI } from '@ai-sdk/google';
        // import { generateText } from 'ai';

        // const google = createGoogleGenerativeAI({ apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY });
        // const model = google('models/gemini-pro'); // Or your preferred model

        // const { text, toolCalls, toolResults, finishReason, usage } = await generateText({
        //   model: model,
        //   prompt: yourConstructedPromptString,
        //   // You might also use system prompts or few-shot examples here.
        // });
        // const generatedQuestionsText = text;
        ```

      - Parse the `generatedQuestionsText` (the LLM's response) to extract the individual question strings. This might involve splitting by newlines if it's a numbered list, or `JSON.parse()` if you prompted for JSON output. Handle potential parsing errors and LLM API errors (the AI SDK might provide error details in the response or throw exceptions).

  5.  Save the LLM-generated questions to the `interview_questions` table, associated with the `session_id` and with `question_order`.
  6.  Update the `interview_sessions` table with `actual_num_questions` (from LLM response) and change status to `ready_to_start` or similar.

- **Response Body (Success - 201 Created):** (Same as before, but questions are now LLM-generated)
  ```json
  {
    "message": "Interview session created and questions generated by LLM.",
    "session_id": "uuid-of-interview-session",
    "questions": [
      {
        "id": "q_uuid_1",
        "question_text": "LLM generated question 1...",
        "order": 1
      },
      {
        "id": "q_uuid_2",
        "question_text": "LLM generated question 2...",
        "order": 2
      }
    ]
  }
  ```
- **Response Body (Error):** (Same as before, ensure LLM API errors from the Vercel AI SDK are handled)

---

## 2. `/api/vapi/assistant` (Detailed for Vapi Webhook)

- **Purpose:** This is the primary webhook endpoint that Vapi will call during an active interview session. It processes user utterances (transcribed by Vapi) and returns the AI's next response/question, managing the interview flow.
- **HTTP Method:** `POST`
- **Route Path:** `/api/vapi/assistant`
- **Authentication:** Vapi will send a specific header (e.g., `Authorization: Bearer <VAPI_SECRET_TOKEN>`) for verification. This secret token must be configured in Vapi and checked by this API route. This is **not** standard user session auth.
- **Request Body (Example - Vapi's format will be specific, refer to Vapi documentation):**
  ```json
  // This is a conceptual Vapi payload. You MUST consult Vapi's documentation
  // for the exact structure of messages like 'assistant-request', 'speech-update', 'hangup', etc.
  {
    "message": {
      "type": "assistant_request", // Indicates Vapi is requesting an assistant response
      "call": {
        // Information about the call
        "id": "vapi-call-uuid",
        "orgId": "your-org-id",
        "assistantId": "your-assistant-id"
      },
      "transcript": [
        // Array of transcript objects, latest is usually most relevant
        {
          "role": "user", // or 'assistant'
          "transcript": "The user's latest spoken words.",
          "timestamp": "iso-timestamp",
          "confidence": 0.95 // Speech recognition confidence
        }
      ],
      // You might need to pass your internal session_id to Vapi when initiating the call,
      // and Vapi might echo it back or you might need to map vapi_call_id to your session_id.
      // Let's assume you pass `forwardedParams` or `metadata` when creating the Vapi call:
      "payload": {
        // This structure depends on how you send custom data to Vapi
        "custom_session_id": "our-internal-interview-session-uuid",
        "current_question_index": 0 // State managed and passed back by Vapi
      }
    }
  }
  ```
- **Core Logic:**
  1.  **Verify Vapi Request:** Check the `Authorization` header for the correct Vapi secret token.
  2.  Extract `custom_session_id` (your internal interview session ID) and `current_question_index` from the request payload (e.g., `message.payload.custom_session_id`).
  3.  If the message type indicates user speech (`message.transcript` is present and relevant):
      - Identify the most recent user transcript.
      - Save the user's answer (`transcript.text`) to the `interview_answers` table, linked to the question at `current_question_index` for the `custom_session_id`.
  4.  **Determine Next Action (based on message type and current state):**
      - If Vapi is requesting the next response (e.g., `message.type === 'assistant_request'` or after user speech):
        - Increment `current_question_index` (or start at 0 if it's the first question).
        - Fetch the next question text from the `interview_questions` table for the `custom_session_id` and the new `current_question_index`.
  5.  **Prepare Response for Vapi:**
      - **If there are more questions:**
        - Construct a response payload that Vapi understands for sending a message.
        - Include the next question text.
        - Include the updated `current_question_index` in the state to be passed back to Vapi.
      - **If no more questions (interview ended):**
        - Construct a response payload for Vapi to say a concluding message (e.g., "Thank you, that concludes our mock interview. Your feedback will be available shortly.").
        - Tell Vapi to end the call (hangup).
        - Update `interview_sessions` status in your database to 'completed'.
        - (Optional, Asynchronous) Trigger the feedback generation process by internally calling `/api/interviews/{sessionId}/feedback` or enqueuing a task.
- **Response Body (Success - 200 OK - Vapi's expected format - MUST align with Vapi docs):**
  - **To send a message/question:**
    ```json
    // Example: Consult Vapi documentation for exact `assistant_response` structure
    {
      "assistant": {
        // Or similar top-level key
        "messages": [
          {
            "type": "text", // or 'speech'
            "role": "assistant",
            "text": "Next interview question goes here..."
          }
        ]
      },
      "control": {
        // Optional, for controlling call flow
        "state": {
          // State to be passed back in next request
          "custom_session_id": "our-internal-interview-session-uuid",
          "current_question_index": 1 // updated index
        }
      }
      // ... other Vapi control flags like `endCall: false`
    }
    ```
  - **To end the call:**
    ```json
    // Example: Consult Vapi documentation for exact hangup structure
    {
      "assistant": {
        "messages": [
          {
            "type": "text",
            "role": "assistant",
            "text": "Thank you for your time."
          }
        ]
      },
      "control": {
        "endCall": true
      }
    }
    ```
- **Response Body (Error):**
  - `400 Bad Request`: Invalid request from Vapi.
  - `401 Unauthorized`: Invalid Vapi token.
  - `500 Internal Server Error`: Database error, logic error.
  - (Vapi might expect a specific error format or just a non-200 status).
  ```json
  { "error": "Error message for internal logging" }
  ```
